{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamental Analysis With Machine Learning in Python\n",
    "\n",
    "### By: Ari Silburt\n",
    "\n",
    "### Date: January 20th, 2017\n",
    "\n",
    "The full code for this analysis can be found at https://github.com/silburt/Machine_Learning/tree/master/Fundamental_Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Efficient Market Hypothesis (EMH) is a financial theory stating that current asset prices reflect all the available information. A direct extension of this theory is that a trading strategy cannot be concocted to consistently beat the market, and future prices cannot be predicted by analyzing prices from the past. It is a hotly debated theory, being supported by many like Burton Malkiel (wrote _A Random Walk Down Wall Street_) and Eugene Fama (credited with founding the theory), while also being contested by many like Andrew Lo and Craig MacKinlay (co-authored the paper _Stock Market Prices Do Not Follow Random Walks_ with >4000 citations).  \n",
    "\n",
    "Many believe that EMH is false, because then money can be made by carefully selecting stocks. Surely you know of someone/some entity that has boasted about analyzing market trends, selecting a few stocks, and earning huge profits. However, this by itself doesn't falsify the EMH. For example, if I have 1,000 people each flip a coin 10 times, there will statistically be at least one person that flipped 10 heads in a row. That person may think they are particularly amazing, when really it's just random processes + statistics at play. In addition, a kind of weird contradiction is that many believe the EMH is false and yet Markov processes are an industry standard in Finance these days (e.g. Black-Scholes).  \n",
    "\n",
    "Regardless of theory, it is an interesting exercise to see whether Machine Learning + Fundamental Analysis can be used to empirically predict future stock prices. Specifically, I will take all the stocks from the Wilshire 5000 index, get their stock prices and fundamental qualities at years $t$ and $t-1$, and see if the stock price at year $t+1$ can be accurately predicted. For concreteness, I will choose $t=2015$, but the code is generalizable to any $t$. I will also cast this problem as a classification problem instead of a regression problem. This means that if a stock increased between years $t$ and $t+1$ the machine learning algorithm should predict $1$, and $0$ otherwise. In contrast, casting this as a regression problem would mean I want to predict _how much_ a stock increased or decreased between $t$ and $t+1$. As you can imagine, this is much more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Getting Stock Prices\n",
    "First we need some stocks. From [here](http://www.beatthemarketanalyzer.com/blog/wilshire-5000-stock-tickers-list/) I got an excel file containing all the stocks from the Wilshire5000 index and converted it to a csv, which can then be easily loaded with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA</td>\n",
       "      <td>Alcoa Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AACC</td>\n",
       "      <td>Asset Accep Cap Corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAI</td>\n",
       "      <td>Airtran Hldgs Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAII</td>\n",
       "      <td>Alabama Aircraft Ind In</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol                  Company\n",
       "0      A     Agilent Technologies\n",
       "1     AA                Alcoa Inc\n",
       "2   AACC     Asset Accep Cap Corp\n",
       "3    AAI        Airtran Hldgs Inc\n",
       "4   AAII  Alabama Aircraft Ind In"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tickers = pd.read_csv('fundamental_analysis/wilshire5000.csv',delimiter=\",\")\n",
    "tickers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need the prices for each stock, which can be pretty easily obtained using the `yahoo finance datareader`, called through `pandas` (along with a fix, since Yahoo! Finance decommissioned their historical data API). The following code gets the first couple stock prices for Agilent Technologies between Jan 1st-8th, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>41.180000</td>\n",
       "      <td>41.310001</td>\n",
       "      <td>40.369999</td>\n",
       "      <td>40.560001</td>\n",
       "      <td>39.430428</td>\n",
       "      <td>1529200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>40.320000</td>\n",
       "      <td>40.459999</td>\n",
       "      <td>39.700001</td>\n",
       "      <td>39.799999</td>\n",
       "      <td>38.691589</td>\n",
       "      <td>2041800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>39.810001</td>\n",
       "      <td>40.020000</td>\n",
       "      <td>39.020000</td>\n",
       "      <td>39.180000</td>\n",
       "      <td>38.088852</td>\n",
       "      <td>2080600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>39.520000</td>\n",
       "      <td>39.810001</td>\n",
       "      <td>39.290001</td>\n",
       "      <td>39.700001</td>\n",
       "      <td>38.594372</td>\n",
       "      <td>3359700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>40.240002</td>\n",
       "      <td>40.980000</td>\n",
       "      <td>40.180000</td>\n",
       "      <td>40.889999</td>\n",
       "      <td>39.751240</td>\n",
       "      <td>2116300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close   Volume\n",
       "Date                                                                      \n",
       "2015-01-02  41.180000  41.310001  40.369999  40.560001  39.430428  1529200\n",
       "2015-01-05  40.320000  40.459999  39.700001  39.799999  38.691589  2041800\n",
       "2015-01-06  39.810001  40.020000  39.020000  39.180000  38.088852  2080600\n",
       "2015-01-07  39.520000  39.810001  39.290001  39.700001  38.594372  3359700\n",
       "2015-01-08  40.240002  40.980000  40.180000  40.889999  39.751240  2116300"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "import fix_yahoo_finance\n",
    "pdr.get_data_yahoo(tickers['Symbol'][0], '2015-01-01', '2015-01-08')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing $t=2015$, we will take the mean Adjusted Close price in January 2015 and January 2014, and ultimately try to predict the mean Adjusted Close price in January 2016. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Financial Data\n",
    "\n",
    "Next, we need to scrape all the relevant fundamental quantities for each stock and process them down into features. [morningstar.com](http://www.morningstar.com/) has an extensive list of \"Key Ratios\" and \"Financials\" for each stock:<br>\n",
    "<img src=\"fundamental_analysis/morningstar.png\", width=700>\n",
    "\n",
    "highlighted in purple is the export button that normal muggles might use to manually download financial data for all 5000 stocks, one by one. However, there's a faster, more efficient solution. We can scrape each stock's financial data into a csv file using the following code snippet:\n",
    "***\n",
    "```python\n",
    "from pattern.web import URL\n",
    "for stock in tickers['Symbol']:\n",
    "    webpage = \"http://financials.morningstar.com/ajax/exportKR2CSV.html?t=%s&culture=en-CA&region=USA&order=asc&r=314562\"%stock\n",
    "    url = URL(webpage)\n",
    "    f = open('%s_keyratios.csv'%(path, stock), 'wb')\n",
    "    f.write(url.download())\n",
    "    f.close()\n",
    "```\n",
    "***\n",
    "the `webpage` variable was obtained by:\n",
    "- Navigating to developer tools under Chrome web browser and clicking the network tab to monitor the ALL tab. \n",
    "- Pressing that export button for a single stock and noticing the corresponding url request sent in the ALL tab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data Arrays\n",
    "Now comes the most difficult part. We need to process all the financial data into `X` and `y` data arrays that a machine learning algorithm can use. In principal it's not difficult, but there's a lot of subtle cleaning and processing that has to happen like:\n",
    "- Converting financial data that are cast in other currencies to USD (you'd think all stocks from the Wilshire 5000 would be in USD already...).\n",
    "- Removing features that are sparsely filled.\n",
    "- Filling NaN values with median feature values.\n",
    "- Taking Year-Over-Year (YOY) features whenever applicable.\n",
    "- Generating new features that are not in morningstar like Debt/Equity, Price/Book, etc.\n",
    "- Casting \n",
    "\n",
    "Performing all these tasks leads to a data array that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stock</th>\n",
       "      <th>Asset Turnover</th>\n",
       "      <th>Asset Turnover (Average)</th>\n",
       "      <th>Asset Turnover (Average) YOY</th>\n",
       "      <th>Asset Turnover YOY</th>\n",
       "      <th>Book Value Per Share * USD</th>\n",
       "      <th>Book Value Per Share * USD YOY</th>\n",
       "      <th>Cap Ex as a % of Sales</th>\n",
       "      <th>Cap Spending USD Mil</th>\n",
       "      <th>Cash &amp; Short-Term Investments</th>\n",
       "      <th>...</th>\n",
       "      <th>Total Stockholders' Equity</th>\n",
       "      <th>Total Stockholders' Equity YOY</th>\n",
       "      <th>Working Capital Ratio</th>\n",
       "      <th>P/E Ratio</th>\n",
       "      <th>P/B Ratio</th>\n",
       "      <th>D/E Ratio</th>\n",
       "      <th>Working Capital Ratio YOY</th>\n",
       "      <th>P/E Ratio YOY</th>\n",
       "      <th>P/B Ratio YOY</th>\n",
       "      <th>D/E Ratio YOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>12.36</td>\n",
       "      <td>0.734403</td>\n",
       "      <td>2.43</td>\n",
       "      <td>-98.0</td>\n",
       "      <td>26.78</td>\n",
       "      <td>...</td>\n",
       "      <td>55.72</td>\n",
       "      <td>1.139002</td>\n",
       "      <td>2.258356</td>\n",
       "      <td>29.630282</td>\n",
       "      <td>2.876726</td>\n",
       "      <td>0.794688</td>\n",
       "      <td>1.153568</td>\n",
       "      <td>1.268805</td>\n",
       "      <td>1.279754</td>\n",
       "      <td>0.761083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAI</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>11.40</td>\n",
       "      <td>1.018060</td>\n",
       "      <td>27.02</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>8.04</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.546073</td>\n",
       "      <td>1.644196</td>\n",
       "      <td>0.126457</td>\n",
       "      <td>0.011404</td>\n",
       "      <td>0.608200</td>\n",
       "      <td>0.992956</td>\n",
       "      <td>1.116002</td>\n",
       "      <td>4.256461</td>\n",
       "      <td>0.651388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAME</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.019231</td>\n",
       "      <td>1.019231</td>\n",
       "      <td>5.05</td>\n",
       "      <td>1.018145</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>4.97</td>\n",
       "      <td>...</td>\n",
       "      <td>32.58</td>\n",
       "      <td>0.991177</td>\n",
       "      <td>1.483239</td>\n",
       "      <td>25.086183</td>\n",
       "      <td>0.943837</td>\n",
       "      <td>2.069368</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>1.238390</td>\n",
       "      <td>1.216320</td>\n",
       "      <td>1.013260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAN</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.976378</td>\n",
       "      <td>0.976378</td>\n",
       "      <td>18.50</td>\n",
       "      <td>1.116476</td>\n",
       "      <td>1.90</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>...</td>\n",
       "      <td>51.40</td>\n",
       "      <td>1.032129</td>\n",
       "      <td>2.057613</td>\n",
       "      <td>12.284312</td>\n",
       "      <td>1.235071</td>\n",
       "      <td>0.945525</td>\n",
       "      <td>1.032922</td>\n",
       "      <td>0.466568</td>\n",
       "      <td>0.719705</td>\n",
       "      <td>0.937991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.968553</td>\n",
       "      <td>0.968553</td>\n",
       "      <td>3.74</td>\n",
       "      <td>1.129909</td>\n",
       "      <td>5.85</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>8.74</td>\n",
       "      <td>...</td>\n",
       "      <td>76.84</td>\n",
       "      <td>1.029061</td>\n",
       "      <td>4.317789</td>\n",
       "      <td>27.779792</td>\n",
       "      <td>6.239312</td>\n",
       "      <td>0.301406</td>\n",
       "      <td>1.093696</td>\n",
       "      <td>1.047728</td>\n",
       "      <td>0.973631</td>\n",
       "      <td>0.888510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Stock  Asset Turnover  Asset Turnover (Average)  \\\n",
       "0     A            0.44                      0.44   \n",
       "1   AAI            0.38                      0.38   \n",
       "2  AAME            0.53                      0.53   \n",
       "3   AAN            1.24                      1.24   \n",
       "4  AAON            1.54                      1.54   \n",
       "\n",
       "   Asset Turnover (Average) YOY  Asset Turnover YOY  \\\n",
       "0                      0.676923            0.676923   \n",
       "1                      0.974359            0.974359   \n",
       "2                      1.019231            1.019231   \n",
       "3                      0.976378            0.976378   \n",
       "4                      0.968553            0.968553   \n",
       "\n",
       "   Book Value Per Share * USD  Book Value Per Share * USD YOY  \\\n",
       "0                       12.36                        0.734403   \n",
       "1                       11.40                        1.018060   \n",
       "2                        5.05                        1.018145   \n",
       "3                       18.50                        1.116476   \n",
       "4                        3.74                        1.129909   \n",
       "\n",
       "   Cap Ex as a % of Sales  Cap Spending USD Mil  \\\n",
       "0                    2.43                 -98.0   \n",
       "1                   27.02                 -18.0   \n",
       "2                    0.19                  -0.0   \n",
       "3                    1.90                 -61.0   \n",
       "4                    5.85                 -21.0   \n",
       "\n",
       "   Cash & Short-Term Investments      ...        Total Stockholders' Equity  \\\n",
       "0                          26.78      ...                             55.72   \n",
       "1                           8.04      ...                            100.00   \n",
       "2                           4.97      ...                             32.58   \n",
       "3                           1.40      ...                             51.40   \n",
       "4                           8.74      ...                             76.84   \n",
       "\n",
       "   Total Stockholders' Equity YOY  Working Capital Ratio  P/E Ratio  \\\n",
       "0                        1.139002               2.258356  29.630282   \n",
       "1                        1.546073               1.644196   0.126457   \n",
       "2                        0.991177               1.483239  25.086183   \n",
       "3                        1.032129               2.057613  12.284312   \n",
       "4                        1.029061               4.317789  27.779792   \n",
       "\n",
       "   P/B Ratio  D/E Ratio  Working Capital Ratio YOY  P/E Ratio YOY  \\\n",
       "0   2.876726   0.794688                   1.153568       1.268805   \n",
       "1   0.011404   0.608200                   0.992956       1.116002   \n",
       "2   0.943837   2.069368                   0.995699       1.238390   \n",
       "3   1.235071   0.945525                   1.032922       0.466568   \n",
       "4   6.239312   0.301406                   1.093696       1.047728   \n",
       "\n",
       "   P/B Ratio YOY  D/E Ratio YOY  \n",
       "0       1.279754       0.761083  \n",
       "1       4.256461       0.651388  \n",
       "2       1.216320       1.013260  \n",
       "3       0.719705       0.937991  \n",
       "4       0.973631       0.888510  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('fundamental_analysis/X.csv')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding target, `y`, is an array of 0s/1s corresponding to whether each stock did/didn't increase between $t$ and $t+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Tyme\n",
    "\n",
    "An article by [forbes](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#32d83d1d6f63) claimed that the average data scientist spend 60% of their time cleaning data, and this project is no exception. After a lot of scraping, cleaning and preparing, we are finally ready to do some machine learning. Here we will use XGBoost (eXtreme Gradient Boosted Decision Trees), a popular and powerful Random Forest classifier. \n",
    "\n",
    "Now we split our data into train and test sets, scale our positive class by `scale_pos_weight` to offset any class imbalances, and perform a random grid search over parameters to tune the hyperparameters on the training set using cross validation:\n",
    "***\n",
    "```python\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "scale_pos_weight = len(y_train[y_train==0])/float(len(y_train[y_train==1]))\n",
    "\n",
    "model = xgb.XGBClassifier(scale_pos_weight=scale_pos_weight)\n",
    "n_cv = 4        #number of cross validation folds per search\n",
    "n_iter = 20     #number of RandomizedSearchCV search iterations\n",
    "param_grid={\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [2,4,8,16],\n",
    "    'min_child_weight': [0.05,0.1,0.2,0.5,1,3],\n",
    "    'max_delta_step': [0,1,5,10],\n",
    "    'colsample_bytree': [0.1,0.5,1],\n",
    "    'gamma': [0,0.2,0.4,0.8],\n",
    "    'n_estimators':[1000],\n",
    "}\n",
    "\n",
    "grid = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=n_iter, cv=n_cv, scoring='roc_auc')\n",
    "grid.fit(X_train,y_train)\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can take the best model and look at the results on the test set:\n",
    "<img src=\"fundamental_analysis/results.png\", width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "- Perform analysis on individual sectors vs. the wilshire 5000 which contains all sectors.\n",
    "- filter by marketcap\n",
    "- Cast the problem as a regression problem instead of classification (i.e. how much did the stock increase/decrease by). \n",
    "- These results aren't good for all possible years/combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe that the EMH is half true - in a vacuum, fundamental analysis can accurately predict the future performance of a stock, but as these measurements are mixed in with external random and chaotic events the predictability decreases. As the chaos and randomness increases, the predictability decreases. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
