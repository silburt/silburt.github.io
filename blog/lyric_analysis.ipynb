{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an LSTM to Generate Song Lyrics in Python\n",
    "\n",
    "### By: Ari Silburt\n",
    "\n",
    "### Date: January 20th, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"col-lg-6 col-sm-7\">\n",
    "<p>\n",
    "Music has a big influence on people. Whether we are simply listening to music as background noise or blocking out the world to listen to a favourite song, it affects our mood and reflects our values. Lyrics play an essential role in music, encoding the feelings, moods and philosophies of the artist. Although every song is different, overarching themes are associated with each genre. \n",
    "</p>\n",
    "<p>\n",
    "In this blog post I will train a neural network in Python to generate lyrics from each of - Country, Electonic Dance Music (EDM), Rock, Pop and Rap - and also extract the broad themes that define each.\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"lyric_analysis/Pink_floyd.jpg\", width=350>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "The code used for this project comes from my [Machine Learning Repo](https://github.com/silburt/Machine_Learning/).  \n",
    "\n",
    "First we need some songs. I picked a few playlists from Spotify: \n",
    "- <i>Country Top Songs of 2000-2017</i> by sjwheat (1567 songs)\n",
    "- <i>Best EDM Playlist on the Planet!</i> by Matt Hoppe (1592 songs)\n",
    "- <i>Greatest Rock Songs Ever</i> by Max John Maybury (1049 songs)\n",
    "- <i>Super Top Hits 2000-2016</i> by Michele Insalata (1603 songs)\n",
    "- <i>BEST of RAP</i> by Andy Mathers (2128 songs)\n",
    "\n",
    "and converted these playlists to csvs [here](https://github.com/watsonbox/exportify), with each csv row containing the artist, song, album, etc. for each entry. Then, I scraped each lyric from the genius.com API using standard Python packages like `BeautifulSoup` and `requests`. A good fraction of songs (20-40% per genre) weren't recognized in the genius.com API due to slightly different song/artist/mix version details between Spotify and genius.com. I also removed songs that had fewer than 15 words (i.e. instrumentals). In total, 1000 songs were used from each genre for the following analysis.\n",
    "\n",
    "In addition, some basic preprocessing was done. Specifically:\n",
    "- All lyrics are converted to lower case.\n",
    "- Accented characters are converted to normal characters.\n",
    "- Punctuation is removed ('!', '?', '.')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Unique Words\n",
    "Before jumping into machine learning, I thought I'd first present the broad themes and emotions of each genre by showing words from each genre with a disproportionately high or low rank (compared to the other genres). By rank, I essentially mean popularity. So for example, '_the_' is the most popular word in country music and has a rank of 1, '_i_' is the second most popular word and has a rank of 2, etc.\n",
    "\n",
    "\n",
    "The left/right tables below shows words with at least a 1.5x higher/lower rank in a given genre. So, for example, the rank of the word \"_we_\" in each genre is: EDM=5, Rap=18, Rock=28, Country=15, Pop=22, and hence it is disproportionately more common in the EDM genre compared to all others.\n",
    "\n",
    "<div class=\"col-lg-6 col-sm-7\">\n",
    "<br>\n",
    "<center> __Disproportionately high rank for given genre__ </center>\n",
    "\n",
    "| EDM | Rap | Rock | Country | Pop | \n",
    "| ------------- |:-------------:| ------------- |:-------------:| ------------- |\n",
    "| we    | got       | well   | little | |\n",
    "| feel  | she       | woman  | every  | |\n",
    "| we're | ni\\*\\*a(s)| soul   | old    | |\n",
    "| our   | they      | people | song   | |\n",
    "| us    | bitch(es) |        | kiss   | |\n",
    "| into  | fuck      |        | town   | |\n",
    "| again | shit      |        | road   | |\n",
    "| light | money     |        | those  | |\n",
    "| fire  | i'mma     |        | | |\n",
    "| fall  | hit       |        | | |\n",
    "| jump  | pussy     |        | | | |\n",
    "\n",
    "</div>\n",
    "<div>\n",
    "<br>\n",
    "<center> __Disproportionately low rank for given genre__ </center>\n",
    "\n",
    "| EDM | Rap | Rock | Country | Pop | \n",
    "| ------------- |:-------------:| ------------- |:-------------:| ------------- |\n",
    "| she   | love   | this   | | |\n",
    "| ain't | we're  | still | | |\n",
    "| her   | you're | even | | |\n",
    "| girl  | our    | | | |\n",
    "| man   | gonna  |      | | |\n",
    "| he    | away   |      | | |\n",
    "| bad   | heart  |      | | |\n",
    "| she's | were   |      | | |\n",
    "| his   | world  |      | | |\n",
    "| crazy | light  |      | | |\n",
    "| him   | eyes   |      | | | |\n",
    "</div>\n",
    "Some interesting trends:\n",
    "- __EDM__: Seems to focus on togetherness (we, our, us), feelings, and drama (fire, light, fall, jump). It also appears very gender neutral, with almost every gendered (pro)noun like she/he, her/him, girl/man disproportionately rare.\n",
    "- __Rap__: (Probably unsurprisingly), it's focused on women, money, and sex. I suppose J-Cole speaks the truth when he says (in the song G.O.M.D.), \"_It's called love, Ni\\*\\*as don't sing about it no more_\". Love seems to be part of a broader trend too, with other intimate words like \"heart\", \"our\", \"light\", \"eyes\" being disproportionately rare.\n",
    "- __Rock__: Seems to retain some overarching 60s and 70s themes, with popular words like \"woman\", \"soul\" and \"people\". Not sure what to make of the disproportionately rare words... \n",
    "- __Country__: Seems to be focused on the simple things in life (old, song, kiss, road, town), and also <i>really</i> likes to miniaturize things - \"little baby\", \"little thing\", \"little kisses\", etc., with \"little\" being the top unique word.\n",
    "- __Pop__: Pop has no disproportionately popular or rare words, which is unsurprising given that pop music draws from all the other genres. Country, rock, rap and EDM all penetrate the Top40s continuously. \n",
    "\n",
    "EDM and rap seem to be the two most distinct genres, having the most number of disproportionately popular/rare words (there are even more disproportionately rare words for both genres not shown here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Neural Network to Generate New Lyrics\n",
    "Text prediction is a really interesting area of Machine Learning at the moment, and one thing we can try is to train a neural network to generate song lyrics. We will be training a [Long Short Term Memory (LSTM) network](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) on each genre to see if it can learn to generate new lyrics with similar properties as the training set. Specifically, given an input sequence of text:  \n",
    "<center>_You think I'm pretty_ </center>\n",
    "we would like the LSTM to be able to finish the sentence with something reasonable, e.g.:\n",
    "<center>_without any makeup on_ </center>\n",
    "Since the LSTM generates outputs predictions probabilistically, running the same input sequence through the LSTM again will produce a different output, e.g.:\n",
    "<center>_sweet like ice cream_ </center>\n",
    "\n",
    "### Training\n",
    "The training data is constructed by taking the 1000 songs for each genre and chopping up the lyrics into input and output sequences of length $L$, with the output sequences offset from the input by one. In total this produces about a million examples (give or take, depending on $L$ and the genre). An example input/output training pair is:  \n",
    "```\n",
    " Input = [y, o, u, \\, t, h, i, n, k, \\, i, ', m, \\, p, r, e, t, t]\n",
    "Output = [o, u, \\, t, h, i, n, k, \\, i, ', m, \\, p, r, e, t, t, y]\n",
    "```\n",
    "and the LSTM is trying to predict each `Output[i]` based off the `Input[i]` value. This is a \"[many-to-many](https://i.stack.imgur.com/b4sus.jpg)\" framework, and I found this _way_ more effective than a many-to-one framework (i.e. trying to predict only the final letter `y` based off the entire `Input` sequence above). This makes sense, as each training example now has $L$ testable predictions per example instead of just one, providing more information during training. \n",
    "\n",
    "Each characeter is then converted to a [one-hot](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) encoding so that it can be used by the network. That is, each unique character is converted to an array with all 0s except one element which has a 1. E.g.:\n",
    "```\n",
    "'a'  -> [1,0,0,...,0]  \n",
    "'b'  -> [0,1,0,...,0]  \n",
    "...  \n",
    "'\\n' -> [0,0,0,...,1]  \n",
    "```\n",
    "In total 45 different one-hot encodings were needed to account for the 26 letters of the alphabet, 10 numbers (0-9), and about 10 extra characters like '?', '!', '\"', etc. To save memory (one-hot encodings get memory-intensive fast), I added a generator to the network which converts the characters to one-hot encodings on the fly.\n",
    "\n",
    "The network itself is built in [Keras](keras.io), and uses a [GRU](https://arxiv.org/pdf/1406.1078.pdf), which is a slightly simpler form of LSTM. The network is incredibly easy to build in Keras:\n",
    "```Python\n",
    "model = Sequential()\n",
    "model.add(GRU(lstm_size, dropout=drop, recurrent_dropout=drop, return_sequences=True,\n",
    "              input_shape=(L, vocab_size)))\n",
    "for i in range(depth - 1):\n",
    "    model.add(GRU(lstm_size, dropout=drop, recurrent_dropout=drop, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
    "\n",
    "decay = lr/epochs\n",
    "optimizer = RMSprop(lr=lr, decay=decay)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "```\n",
    "where:  \n",
    "- `vocab_size` are the number of unique characters.\n",
    "- `L` is the length of the input and output sequences, $L$.\n",
    "- `lstm_size` are the number of nodes in the LSTM.\n",
    "- `drop` is the [dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) fraction. \n",
    "- `depth` is the total depth of the network. \n",
    "- `lr` is the learning rate.\n",
    "\n",
    "With the exception of `vocab_size`, all the above variables are tunable hyperparameters. In addition, another key tunable hyperparameter is `batch_size`, which determines how many examples are seen per gradient update. Splitting the data into Training/Validation sets using an 80/20 split, these hyperparameters were tuned for 50 epochs over the following ranges:\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "### Results\n",
    "\n",
    "### Word vs. Character-level LSTMs\n",
    "Although I tried training an LSTM on at both the character and word level, I had way more success with the character level. [Others](https://groups.google.com/d/msg/keras-users/Y_FG_YEkjXs/TD_j9b532kwJ) seem to have had similar experiences. Overall the word-prediction is a more difficult problem, since:\n",
    "- Due to the vast number of different words, the LSTM has to learn orders of magnitude more correlations than the character-level with order of magnitude fewer examples for each. The variable size can be reduced by removing rare words, but then other complexities arise and it doesn't improve things so much.\n",
    "\n",
    "In addition, word-level LSTMs appear to be less flexible. In principle, characters can be combined to produce any word in the dictionary, but words can only be combined to form sentences. In the current LSTM framework, it's impossible to produce a new word not in your training set, severely limiting the space of possible output sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
